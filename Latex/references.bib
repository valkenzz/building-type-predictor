%https://crdig.ulaval.ca/apprenez-en-plus-sur-le-projet-oracle-2/
@article{badard2021geomatique,
  title = {Conception d'une base de données sur les b{\^a}timents pour améliorer la connaissance des risques liés aux inondations},
  author = {Badard, Thierry and Janssens-Coron, Eric and Engélinius, Jonathan and Malet, Xavier},
  journal = {Géomatique},
  edition = {Revue de l’Ordre des Arpenteurs‐Géomètres du Québec},
  volume = {48},
  number = {2},
  pages = {7--13},
  year = {2021},
  month = {Autumn}
}

@article{wurm2015,
author = {Wurm, Michael and Schmitt, Andreas and Taubenböck, Hannes},
year = {2015},
month = {08},
pages = {1-12},
title = {Building Types’ Classification Using Shape-Based Features and Linear Discriminant Functions},
volume = {9},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
doi = {10.1109/JSTARS.2015.2465131}
}

@article{lloyd2020,
author = {Lloyd, Christopher and Sturrock, Hugh and Jochem, Warren and Lázár, Attila and Tatem, Andrew},
year = {2020},
month = {11},
pages = {3847},
title = {Using GIS and Machine Learning to Classify Residential Status of Urban Buildings in Low and Middle Income Settings},
volume = {12},
journal = {Remote Sensing},
doi = {10.3390/rs12233847}
}

@article{belgiu2014,
author = {Belgiu, Mariana and Tomljenovic, Ivan and Lampoltshammer, Thomas and Blaschke, Thomas and Höfle, Bernhard},
year = {2014},
month = {02},
pages = {1347-1366},
title = {Ontology-Based Classification of Building Types Detected from Airborne Laser Scanning Data},
volume = {6},
journal = {Remote Sensing},
doi = {10.3390/rs6021347}
}

@article{evans2017,
author = {Evans, Stephen and Liddiard, Rob and Steadman, Philip},
year = {2017},
month = {12},
pages = {1-17},
title = {Modelling a whole building stock: domestic, non-domestic and mixed use},
volume = {47},
journal = {Building Research \& Information},
doi = {10.1080/09613218.2017.1410424}
}

%check citation DONE
@article{shimodaira2000covariate,
author = {Shimodaira, Hidetoshi},
year = {2000},
month = {10},
pages = {227-244},
title = {Improving predictive inference under covariate shift by weighting the log-likelihood function},
volume = {90},
journal = {Journal of Statistical Planning and Inference},
doi = {10.1016/S0378-3758(00)00115-4}
}

@misc{villedequebec2025,
  author       = {{Ville de Québec}},
  title        = {Organisation – Ville de Québec},
  howpublished = {\url{https://www.donneesquebec.ca/organisation/ville-de-quebec/}},
  year         = {2025},
  note         = {Accessed: 2025-07-08}
}

@misc{opencanada_qc2025,
  author       = {{Government of Canada}},
  title        = {Government and Municipalities of Québec},
  howpublished = {\url{https://open.canada.ca/data/organization/qc}},
  year         = {2025},
  note         = {Accessed: 2025-07-08}
}

@misc{ms2023globalmlbuildingfootprints,
  author       = {{Microsoft Corporation}},
  title        = {GlobalMLBuildingFootprints},
  howpublished = {\url{https://github.com/microsoft/GlobalMLBuildingFootprints}},
  year         = {2023},
  note         = {Accessed: 2025-07-08}
}

@misc{openstreetmap_api2025,
  author       = {{OpenStreetMap contributors}},
  title        = {API – OpenStreetMap Wiki},
  howpublished = {\url{https://wiki.openstreetmap.org/wiki/API}},
  year         = {2025},
  note         = {Accessed: 2025-07-08}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

%check citation DONE
@article{lance1967canberra,
    author = {Lance, G. N. and Williams, W. T.},
    title = {A general theory of classificatory sorting strategies: II. Clustering systems},
    journal = {The Computer Journal},
    volume = {10},
    number = {3},
    pages = {271-277},
    year = {1967},
    month = {01},
    abstract = {Current clustering programs (i.e., non-hierarchical classificatory programs) are examined, with particular reference to the internal consistency of the methods used for initiation, allocation and reallocation. It is shown that almost all existing methods are open to serious objection, and that no method fully exploits the potentialities of such systems. The desirable properties of a clustering program are examined de novo, and suggestions made for optimum lines of further development.},
    issn = {0010-4620},
    doi = {10.1093/comjnl/10.3.271},
    url = {https://doi.org/10.1093/comjnl/10.3.271},
    eprint = {https://academic.oup.com/comjnl/article-pdf/10/3/271/1333425/100271.pdf},
}

%check citation DONE
@article{cortes1995svm,
  author    = {Corinna Cortes and Vladimir Vapnik},
  title     = {Support-vector networks},
  journal   = {Machine Learning},
  year      = {1995},
  volume    = {20},
  number    = {3},
  pages     = {273--297},
  month     = sep,
  abstract  = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  issn      = {1573-0565},
  doi       = {10.1007/BF00994018},
  url       = {https://doi.org/10.1007/BF00994018},
}

%check citation DONE
@inproceedings{quinlan1986id3,
  author    = {J. R. Quinlan},
  title     = {Induction of decision trees},
  journal   = {Machine Learning},
  year      = {1986},
  volume    = {1},
  number    = {1},
  pages     = {81--106},
  month     = mar,
  abstract  = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
  issn      = {1573-0565},
  doi       = {10.1007/BF00116251},
  url       = {https://doi.org/10.1007/BF00116251},
}

%check citation DONE
@article{rumelhart1986mlp,
  author    = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  title     = {Learning representations by back-propagating errors},
  journal   = {Nature},
  year      = {1986},
  volume    = {323},
  number    = {6088},
  pages     = {533--536},
  month     = oct,
  abstract  = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.},
  issn      = {1476-4687},
  doi       = {10.1038/323533a0},
  url       = {https://doi.org/10.1038/323533a0},
}

%check citation DONE
@article{breiman2001randomforest,
  author    = {Leo Breiman},
  title     = {Random Forests},
  journal   = {Machine Learning},
  year      = {2001},
  volume    = {45},
  number    = {1},
  pages     = {5--32},
  month     = oct,
  abstract  = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  issn      = {1573-0565},
  doi       = {10.1023/A:1010933404324},
  url       = {https://doi.org/10.1023/A:1010933404324},
}

%check citation DONE
@article{friedman2001gradient,
 ISSN = {00905364, 21688966},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 urldate = {2025-07-08},
 volume = {29},
 year = {2001}
}

@article{gregorutti2017correlation,
  title={Correlation and variable importance in random forests},
  author={Gregorutti, Baptiste and Michel, Bertrand and Saint-Pierre, Philippe},
  journal={Statistics and Computing},
  volume={27},
  pages={659--678},
  year={2017}
}
